{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Structural Factor Analysis of benchmark for Over-Refusal Behavior Based on Varies LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Gemma3-4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Llama3.1-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Qwen3-4b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Gemini-2.5-flash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Deepseek-V3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:00:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: GPU Acceleration Status: Disabled (Device: cpu)\n",
      "[INFO]: Total 5 models configured for analysis.\n",
      "------------------------------------------------------------\n",
      "[INFO]: Chart save directory created/verified: /Users/ziyin/Workspace/MyLabs/labs2025s2/capstone/code/USYD-25S2-Capstone-CS62-2/evaluation/images_2\n",
      "\n",
      "==================== Starting Analysis for Model: deepseek-v3.2 (Slug: deepseekv32) ====================\n",
      ">> Computing features for EN using column 'English'...\n",
      "[INFO]: Loading Stanza Pipeline for language 'en'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e6ee772fc5460ba2e21560b3a239c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-10 17:00:24 INFO: Downloaded file to /Users/ziyin/stanza_resources/resources.json\n",
      "2025-11-10 17:00:24 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-11-10 17:00:24 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "=================================\n",
      "\n",
      "2025-11-10 17:00:24 INFO: Using device: cpu\n",
      "2025-11-10 17:00:24 INFO: Loading: tokenize\n",
      "2025-11-10 17:00:25 INFO: Loading: mwt\n",
      "2025-11-10 17:00:25 INFO: Loading: pos\n",
      "2025-11-10 17:00:25 INFO: Loading: lemma\n",
      "2025-11-10 17:00:26 INFO: Loading: depparse\n",
      "2025-11-10 17:00:26 INFO: Done loading processors!\n",
      "  4%|â–         | 21/600 [00:09<04:08,  2.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 435\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Iterate through the models\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_slug, model_name \u001b[38;5;129;01min\u001b[39;00m MODEL_MAPPING\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 435\u001b[0m     chart \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_single_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_slug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBASE_DATA_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chart:\n\u001b[1;32m    438\u001b[0m         all_charts\u001b[38;5;241m.\u001b[39mappend(chart)\n",
      "Cell \u001b[0;32mIn[1], line 333\u001b[0m, in \u001b[0;36mprocess_single_model\u001b[0;34m(model_slug, model_name, base_dir)\u001b[0m\n\u001b[1;32m    331\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _id, text \u001b[38;5;129;01min\u001b[39;00m tqdm(df_cn[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, text_col]]\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(df_cn)):\n\u001b[0;32m--> 333\u001b[0m     feats \u001b[38;5;241m=\u001b[39m \u001b[43mstanza_features_for_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feats\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m    336\u001b[0m df_f \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "Cell \u001b[0;32mIn[1], line 164\u001b[0m, in \u001b[0;36mstanza_features_for_text\u001b[0;34m(text, nlp)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharacter_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_len\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep_depth_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep_distance_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_clause_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunct_complex_count\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype_token_ratio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlexical_information_entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    162\u001b[0m     }\n\u001b[0;32m--> 164\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m sents \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39msentences\n\u001b[1;32m    166\u001b[0m sent_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sents)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/pipeline/lemma_processor.py:94\u001b[0m, in \u001b[0;36mLemmaProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     92\u001b[0m edits \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(seq2seq_batch):\n\u001b[0;32m---> 94\u001b[0m     ps, es \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq2seq_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ps\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m es \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/models/lemma/trainer.py:114\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, beam_size, vocab)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    113\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m preds, edit_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m [vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m preds] \u001b[38;5;66;03m# unmap to tokens\u001b[39;00m\n\u001b[1;32m    116\u001b[0m pred_seqs \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mprune_decoded_seqs(pred_seqs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:301\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict\u001b[0;34m(self, src, src_mask, pos, beam_size, raw, never_decode_unk)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Predict with beam search. \"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beam_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_greedy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_decode_unk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnever_decode_unk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m enc_inputs, batch_size, src_lens, src_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(src, src_mask, pos, raw)\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# (1) encode source\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:279\u001b[0m, in \u001b[0;36mSeq2SeqModel.predict_greedy\u001b[0;34m(self, src, src_mask, pos, raw, never_decode_unk)\u001b[0m\n\u001b[1;32m    276\u001b[0m output_seqs \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m total_done \u001b[38;5;241m<\u001b[39m batch_size \u001b[38;5;129;01mand\u001b[39;00m max_len \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_dec_len:\n\u001b[0;32m--> 279\u001b[0m     log_probs, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_decode_unk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnever_decode_unk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m log_probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput must have 1-step of output.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m     _, preds \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/models/common/seq2seq_model.py:137\u001b[0m, in \u001b[0;36mSeq2SeqModel.decode\u001b[0;34m(self, dec_inputs, hn, cn, ctx, ctx_mask, src, never_decode_unk)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Decode a step, based on context encoding and source context states.\"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m dec_hidden \u001b[38;5;241m=\u001b[39m (hn, cn)\n\u001b[0;32m--> 137\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_logattn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy:\n\u001b[1;32m    139\u001b[0m     h_out, dec_hidden, log_attn \u001b[38;5;241m=\u001b[39m decoder_output\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/stanza/models/common/seq2seq_modules.py:227\u001b[0m, in \u001b[0;36mLSTMAttention.forward\u001b[0;34m(self, input, hidden, ctx, ctx_mask, return_logattn)\u001b[0m\n\u001b[1;32m    225\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[0;32m--> 227\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     hy, cy \u001b[38;5;241m=\u001b[39m hidden\n\u001b[1;32m    229\u001b[0m     h_tilde, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_layer(hy, ctx, mask\u001b[38;5;241m=\u001b[39mctx_mask, return_logattn\u001b[38;5;241m=\u001b[39mreturn_logattn)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/Capstone/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1704\u001b[0m, in \u001b[0;36mLSTMCell.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1702\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[0;32m-> 1704\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1705\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   1714\u001b[0m     ret \u001b[38;5;241m=\u001b[39m (ret[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), ret[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from statistics import mean\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "# --- GLOBAL CONFIGURATION ---\n",
    "# Slug-to-Professional Name mapping for file handling vs. display\n",
    "MODEL_MAPPING: Dict[str, str] = {\n",
    "    \"deepseekv32\": \"deepseek-v3.2\",\n",
    "    \"llama318b\": \"llama3.1-8b\",\n",
    "    \"qwen34b\": \"qwen3-4b\",\n",
    "    \"gemini25flash\": \"gemini-2.5-flash\",\n",
    "    \"gemma34b\": \"gemma3-4b\"\n",
    "}\n",
    "\n",
    "# Parent directory where your data files are located\n",
    "BASE_DATA_DIR = \"../data/label_fusion\"\n",
    "\n",
    "MODEL_NAME_FALLBACK = \"Unknown-LLM\"\n",
    "# --- END GLOBAL CONFIGURATION ---\n",
    "\n",
    "\n",
    "# --- INITIALIZATION AND UTILITY FUNCTIONS ---\n",
    "\n",
    "def _imp(name):\n",
    "    \"\"\"Safely import a module, prompting for installation if it fails.\"\"\"\n",
    "    try:\n",
    "        return importlib.import_module(name)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN]: Please ensure to pip install {name}\")\n",
    "        raise\n",
    "\n",
    "# Initialize necessary libraries\n",
    "try:\n",
    "    stanza = _imp(\"stanza\")\n",
    "    pd = _imp(\"pandas\")\n",
    "    np = _imp(\"numpy\")\n",
    "    alt = _imp(\"altair\")\n",
    "except Exception:\n",
    "    print(\"[ERROR]: Critical dependencies (stanza, pandas, numpy, altair) are missing. Please install them.\")\n",
    "\n",
    "# Altair Configuration\n",
    "alt.data_transformers.disable_max_rows()\n",
    "alt.renderers.enable(\"default\")\n",
    "\n",
    "# Check GPU availability and configure device\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "DEVICE = 'cuda' if USE_GPU else 'cpu'\n",
    "print(f\"[INFO]: GPU Acceleration Status: {'Enabled' if USE_GPU else 'Disabled'} (Device: {DEVICE})\")\n",
    "\n",
    "# Stanza NLP Pipeline cache\n",
    "_NLP_CACHE = {}\n",
    "\n",
    "def get_nlp(lang_code: str):\n",
    "    \"\"\"Get Stanza NLP Pipeline instance, with memory cache and GPU/CPU configuration.\"\"\"\n",
    "    if lang_code not in _NLP_CACHE:\n",
    "        print(f\"[INFO]: Loading Stanza Pipeline for language '{lang_code}'...\")\n",
    "        try:\n",
    "            _NLP_CACHE[lang_code] = stanza.Pipeline(\n",
    "                lang_code,\n",
    "                processors='tokenize,pos,lemma,depparse',\n",
    "                tokenize_no_ssplit=False,\n",
    "                use_gpu=USE_GPU,\n",
    "                device=DEVICE\n",
    "            )\n",
    "        except Exception:\n",
    "            print(f\"[INFO]: Downloading language model for '{lang_code}'...\")\n",
    "            stanza.download(lang_code)\n",
    "            _NLP_CACHE[lang_code] = stanza.Pipeline(\n",
    "                lang_code,\n",
    "                processors='tokenize,pos,lemma,depparse',\n",
    "                tokenize_no_ssplit=False,\n",
    "                use_gpu=USE_GPU,\n",
    "                device=DEVICE\n",
    "            )\n",
    "    return _NLP_CACHE[lang_code]\n",
    "\n",
    "def release_nlp(lang_code: str):\n",
    "    \"\"\"[Memory Cleanup Mechanism] Explicitly release Stanza Pipeline memory and CUDA cache.\"\"\"\n",
    "    if lang_code in _NLP_CACHE:\n",
    "        print(f\"[INFO]: Releasing Stanza Pipeline for language '{lang_code}' and clearing GPU memory...\")\n",
    "        del _NLP_CACHE[lang_code]\n",
    "        _NLP_CACHE.pop(lang_code, None)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        if USE_GPU and torch.cuda.is_available():\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"[INFO]: PyTorch CUDA cache cleared.\")\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN]: Failed to clear CUDA cache: {e}\")\n",
    "    else:\n",
    "        print(f\"[INFO]: Stanza Pipeline for '{lang_code}' was not found in cache.\")\n",
    "\n",
    "# --- LINGUISTIC FEATURE EXTRACTION FUNCTIONS ---\n",
    "CN_COMPLEX_PUNCT = re.compile(r\"[ï¼›ï¼šâ€”â€”â€¦â€”]\")\n",
    "SUBORDINATE_TAGS = {\"mark\", \"advcl\", \"acl\", \"ccomp\", \"xcomp\", \"dep\", \"parataxis\"}\n",
    "\n",
    "def count_complex_punct(text: str) -> int:\n",
    "    return len(CN_COMPLEX_PUNCT.findall(str(text)))\n",
    "\n",
    "def unigram_entropy(tokens):\n",
    "    if not tokens: return 0.0\n",
    "    cnt = Counter(tokens)\n",
    "    n = len(tokens)\n",
    "    ent = 0.0\n",
    "    for c in cnt.values():\n",
    "        p = c / n\n",
    "        ent -= p * math.log(p + 1e-12)\n",
    "    return float(ent)\n",
    "\n",
    "def type_token_ratio(tokens):\n",
    "    return (len(set(tokens)) / len(tokens)) if tokens else 0.0\n",
    "\n",
    "def compute_dep_tree_depth(sent):\n",
    "    children = {}\n",
    "    for w in sent.words:\n",
    "        try: head_id = int(w.head)\n",
    "        except (ValueError, TypeError): continue\n",
    "        children.setdefault(head_id, []).append(w.id)\n",
    "    def dfs(node_id, depth):\n",
    "        if node_id not in children: return depth\n",
    "        return max(dfs(ch, depth + 1) for ch in children[node_id])\n",
    "    depths = [dfs(ch, 1) for ch in children.get(0, [])] or [1]\n",
    "    return max(depths)\n",
    "\n",
    "def compute_dep_distance_mean(sent):\n",
    "    if not sent.words: return 0.0\n",
    "    dists = []\n",
    "    for w in sent.words:\n",
    "        if w.head is not None and w.id is not None:\n",
    "            try:\n",
    "                head_id = int(w.head)\n",
    "                word_id = int(w.id)\n",
    "                if head_id != 0: dists.append(abs(word_id - head_id))\n",
    "            except (ValueError, TypeError): continue\n",
    "    return mean(dists) if dists else 0.0\n",
    "\n",
    "def compute_sub_clause_count(sent):\n",
    "    return sum(1 for w in sent.words if (w.deprel or '').lower() in SUBORDINATE_TAGS)\n",
    "\n",
    "def stanza_features_for_text(text: str, nlp):\n",
    "    text = str(text or \"\").strip()\n",
    "    if not text:\n",
    "        return {\n",
    "            \"character_len\": 0, \"sentence_count\": 0, \"token_len\": 0,\n",
    "            \"dep_depth_mean\": 0.0, \"dep_distance_mean\": 0.0,\n",
    "            \"sub_clause_count\": 0, \"punct_complex_count\": 0,\n",
    "            \"type_token_ratio\": 0.0, \"lexical_information_entropy\": 0.0\n",
    "        }\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sents = doc.sentences\n",
    "    sent_count = len(sents)\n",
    "    tok_len = sum(len(s.words) for s in sents)\n",
    "\n",
    "    dep_depths = [compute_dep_tree_depth(s) for s in sents] if sents else [0]\n",
    "    dep_depth_mean = mean(dep_depths) if dep_depths else 0.0\n",
    "\n",
    "    dep_distance_means = [compute_dep_distance_mean(s) for s in sents] if sents else [0.0]\n",
    "    dep_distance_mean = mean(dep_distance_means) if dep_distance_means else 0.0\n",
    "\n",
    "    sub_clause_total = sum(compute_sub_clause_count(s) for s in sents)\n",
    "    tokens = [w.text for s in sents for w in s.words]\n",
    "\n",
    "    return {\n",
    "        \"character_len\": len(text),\n",
    "        \"sentence_count\": sent_count,\n",
    "        \"token_len\": tok_len,\n",
    "        \"dep_depth_mean\": float(dep_depth_mean),\n",
    "        \"dep_distance_mean\": float(dep_distance_mean),\n",
    "        \"sub_clause_count\": int(sub_clause_total),\n",
    "        \"punct_complex_count\": int(count_complex_punct(text)),\n",
    "        \"type_token_ratio\": float(type_token_ratio(tokens)),\n",
    "        \"lexical_information_entropy\": float(unigram_entropy(tokens)),\n",
    "    }\n",
    "# --- END LINGUISTIC FEATURE EXTRACTION FUNCTIONS ---\n",
    "\n",
    "# --- PLOTTING UTILITY FUNCTIONS ---\n",
    "\n",
    "BINS = 20\n",
    "\n",
    "def _domain_x(arr):\n",
    "    if len(arr) == 0: return None\n",
    "    min_val, max_val = np.nanmin(arr), np.nanmax(arr)\n",
    "    padding = (max_val - min_val) * 0.05\n",
    "    return [min_val - padding, max_val + padding]\n",
    "\n",
    "def _domain_y_max(vals_list, bins):\n",
    "    if not vals_list or all(v.size == 0 for v in vals_list): return [0, 10]\n",
    "    max_count = 0\n",
    "    for vals in vals_list:\n",
    "        if vals.size > 0:\n",
    "            hist, _ = np.histogram(vals, bins=bins, range=(np.nanmin(vals), np.nanmax(vals)))\n",
    "            max_count = max(max_count, np.max(hist))\n",
    "\n",
    "    return [0, int(max_count * 1.8) + 1]\n",
    "\n",
    "\n",
    "def layered_hist_with_labels(df, label_col, x_field, title, bins, width, height, x_extent=None, y_domain=None):\n",
    "    \"\"\"\n",
    "    [å·²ä¿®æ”¹] Generates a histogram focused ONLY on the 'refuse' data points.\n",
    "    \"\"\"\n",
    "    if x_field not in df.columns or label_col not in df.columns:\n",
    "        return alt.Chart(pd.DataFrame()).mark_text(text=f\"Data Missing: {x_field}\").properties(title=title, width=width, height=height)\n",
    "\n",
    "    # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šåªè¿‡æ»¤ 'refuse' æ•°æ®\n",
    "    df_filtered = df[df[label_col] == 'refuse'].copy()\n",
    "\n",
    "    if df_filtered.empty:\n",
    "        return alt.Chart(pd.DataFrame()).mark_text(text=f\"No Refusal Data for: {x_field}\").properties(title=title, width=width, height=height)\n",
    "\n",
    "    base = alt.Chart(df_filtered).properties(title=title + \" (Refusal Prompts Only)\")\n",
    "\n",
    "    # ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œè®¾ç½®å›ºå®šé¢œè‰²\n",
    "    histogram = base.mark_bar(opacity=0.8, color='red').encode(\n",
    "        x=alt.X(x_field, bin=alt.Bin(extent=x_extent, step=(x_extent[1] - x_extent[0]) / bins if x_extent and bins else None), title=x_field, scale=alt.Scale(domain=x_extent)),\n",
    "        y=alt.Y('count()', title='Refusal Count', scale=alt.Scale(domain=y_domain)),\n",
    "        tooltip=[alt.Tooltip('count()', title='Refusal Count')]\n",
    "    )\n",
    "\n",
    "    # ğŸ”´ å…³é”®ä¿®æ”¹ï¼šåªè®¡ç®— 'refuse' ç»„çš„å¹³å‡å€¼\n",
    "    mean_refuse = df_filtered[x_field].mean()\n",
    "\n",
    "    # åªç»˜åˆ¶ 'refuse' ç»„çš„å¹³å‡å€¼çº¿\n",
    "    rule_refuse = alt.Chart(pd.DataFrame({'mean': [mean_refuse]})).mark_rule(color='black', strokeDash=[5, 5]).encode(\n",
    "        x=alt.X('mean:Q', scale=alt.Scale(domain=x_extent)),\n",
    "        tooltip=[alt.Tooltip('mean', format='.2f', title='Mean (Refuse)')]\n",
    "    )\n",
    "\n",
    "    return (histogram + rule_refuse).properties(width=width, height=height)\n",
    "\n",
    "# --- CORE PROCESSING FUNCTION ---\n",
    "def process_single_model(model_slug: str, model_name: str, base_dir: str):\n",
    "    \"\"\"\n",
    "    Processes a single model's data, extracts features (with caching and GPU cleanup), and generates plots.\n",
    "    \"\"\"\n",
    "    print(f\"\\n==================== Starting Analysis for Model: {model_name} (Slug: {model_slug}) ====================\")\n",
    "\n",
    "    # 1. Dynamic Path Determination\n",
    "    file_name = f\"test_{model_slug}_on_local_data_results_labeled.csv\"\n",
    "    CSV_PATH = Path(base_dir) / file_name\n",
    "\n",
    "    if not CSV_PATH.exists():\n",
    "        print(f\"[ERROR]: Data file not found for {model_name}: {CSV_PATH}\")\n",
    "        return\n",
    "\n",
    "    # ç¼“å­˜è·¯å¾„æŒ‡å‘å½“å‰ç›®å½•\n",
    "    CACHE_PATH = Path(f\"{model_slug}_features_cache.csv\")\n",
    "\n",
    "    # 2. Data Loading and Preprocessing\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    def find_col(suffix_regex):\n",
    "        for c in df.columns:\n",
    "            if re.search(suffix_regex, c, flags=re.I): return c\n",
    "        return None\n",
    "\n",
    "    # Dynamically find result columns (ä½¿ç”¨ ^...$ è¿›è¡Œç²¾ç¡®åŒ¹é…)\n",
    "    TEXT_EN = find_col(r\"^English$\")\n",
    "    TEXT_CN = find_col(r\"^Chinese$\")\n",
    "    TEXT_MIX = find_col(r\"^Mixed$\")\n",
    "    LABEL_EN = \"Final_Label_EN\"\n",
    "    LABEL_CN = \"Final_Label_CN\"\n",
    "    LABEL_MIX = \"Final_Label_MIX\"\n",
    "\n",
    "    if not any(c in df.columns for c in [TEXT_EN, TEXT_CN, TEXT_MIX]):\n",
    "        print(f\"[ERROR]: Could not find required 'result' columns for {model_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    for lab in [LABEL_EN, LABEL_CN, LABEL_MIX]:\n",
    "        if lab in df.columns:\n",
    "            df[lab] = df[lab].astype(str).str.lower().str.strip()\n",
    "\n",
    "    if \"id\" not in df.columns:\n",
    "        df = df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "    df = df.rename(columns={\"Rewrite Method\": \"method\", \"Category\": \"category\"}, errors='ignore')\n",
    "    df_cn = df.copy()\n",
    "\n",
    "    # 3. Feature Extraction (with persistent cache and cleanup)\n",
    "    VARIANTS: List[Tuple[str, str, str, str]] = []\n",
    "    if TEXT_EN and TEXT_EN in df_cn.columns:\n",
    "        VARIANTS.append((\"EN\", TEXT_EN, LABEL_EN, \"en\"))\n",
    "    if TEXT_CN and TEXT_CN in df_cn.columns:\n",
    "        VARIANTS.append((\"CN\", TEXT_CN, LABEL_CN, \"zh\"))\n",
    "    if TEXT_MIX and TEXT_MIX in df_cn.columns:\n",
    "        VARIANTS.append((\"MIX\", TEXT_MIX, LABEL_MIX, \"zh\"))\n",
    "\n",
    "    df_feat: pd.DataFrame | None = None\n",
    "\n",
    "    # Try loading from cache\n",
    "    if CACHE_PATH.exists():\n",
    "        try:\n",
    "            df_feat_cached = pd.read_csv(CACHE_PATH)\n",
    "            print(f\"[INFO]: Features loaded successfully from cache: {CACHE_PATH}\")\n",
    "            required_cols = [f\"dep_depth_mean_{v[0]}\" for v in VARIANTS] + [f\"lexical_information_entropy_{v[0]}\" for v in VARIANTS]\n",
    "\n",
    "            # Integrity check\n",
    "            if all(col in df_feat_cached.columns for col in required_cols) and len(df_feat_cached) == len(df_cn):\n",
    "                df_feat_cached[\"id\"] = pd.to_numeric(df_feat_cached[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "                df_cn[\"id\"] = pd.to_numeric(df_cn[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "                df_feat = df_feat_cached\n",
    "            else:\n",
    "                 print(\"[WARN]: Cache file is incomplete or outdated. Recalculating features.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR]: Failed to read cache file {CACHE_PATH}. Recalculating features. Error: {e}\")\n",
    "\n",
    "    if df_feat is None:\n",
    "        feature_frames = []\n",
    "        df_cn[\"id\"] = pd.to_numeric(df_cn[\"id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        for name, text_col, label_col, lang_code in VARIANTS:\n",
    "            print(f\">> Computing features for {name} using column '{text_col}'...\")\n",
    "\n",
    "            # Load Stanza model\n",
    "            nlp = get_nlp(lang_code)\n",
    "\n",
    "            rows = []\n",
    "            for _id, text in tqdm(df_cn[[\"id\", text_col]].itertuples(index=False, name=None), total=len(df_cn)):\n",
    "                feats = stanza_features_for_text(text, nlp)\n",
    "                rows.append({f\"{k}_{name}\": v for k, v in feats.items()})\n",
    "\n",
    "            df_f = pd.DataFrame(rows)\n",
    "            df_f[\"id\"] = df_cn[\"id\"]\n",
    "            df_f.drop(columns=[c for c in df_f.columns if c.startswith('id_')], inplace=True, errors='ignore')\n",
    "\n",
    "            if label_col in df_cn.columns:\n",
    "                df_f = df_f.merge(df_cn[[\"id\", label_col]], on=\"id\", how='left')\n",
    "\n",
    "            feature_frames.append(df_f)\n",
    "\n",
    "            # [CRITICAL STEP: MEMORY CLEANUP] Release GPU memory after processing a language model\n",
    "            release_nlp(lang_code)\n",
    "\n",
    "        # Merge features from all languages\n",
    "        if len(feature_frames) == 1:\n",
    "            df_feat = feature_frames[0].copy()\n",
    "        else:\n",
    "            df_feat = reduce(lambda left, right: pd.merge(left, right, on='id', how='outer'), feature_frames)\n",
    "\n",
    "        # Save features to cache file\n",
    "        try:\n",
    "            df_feat.to_csv(CACHE_PATH, index=False)\n",
    "            print(f\"[INFO]: Features saved to cache: {CACHE_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN]: Could not save features to cache file {CACHE_PATH}. Error: {e}\")\n",
    "\n",
    "    # 4. Plotting\n",
    "\n",
    "    # Define feature column names\n",
    "    DEP_EN  = f\"dep_depth_mean_EN\"\n",
    "    ENT_EN  = f\"lexical_information_entropy_EN\"\n",
    "    DEP_CN  = f\"dep_depth_mean_CN\"\n",
    "    ENT_CN  = f\"lexical_information_entropy_CN\"\n",
    "    DEP_MIX = f\"dep_depth_mean_MIX\"\n",
    "    ENT_MIX = f\"lexical_information_entropy_MIX\"\n",
    "\n",
    "    dep_cols = [DEP_EN, DEP_CN, DEP_MIX]\n",
    "    ent_cols = [ENT_EN, ENT_CN, ENT_MIX]\n",
    "\n",
    "    def get_global_extents(df_feat: pd.DataFrame, dep_cols: List[str], ent_cols: List[str]):\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®¡ç®—å…¨å±€èŒƒå›´æ—¶ï¼Œä»ç„¶ä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬ 'answer'ï¼‰ï¼Œä»¥ç¡®ä¿æ‰€æœ‰å›¾è¡¨çš„ X è½´å’Œ Y è½´èŒƒå›´ä¸€è‡´\n",
    "        vals_dep = [df_feat[col].dropna().values for col in dep_cols if col in df_feat.columns]\n",
    "        valid_vals_dep = [v for v in vals_dep if v.size > 0]\n",
    "        x_extent_dep = _domain_x(np.concatenate(valid_vals_dep)) if valid_vals_dep and np.concatenate(valid_vals_dep).size > 0 else None\n",
    "\n",
    "        # âš ï¸ Yè½´èŒƒå›´éœ€è¦é‡æ–°è®¡ç®—ï¼Œå› ä¸ºç°åœ¨åªæ˜¾ç¤º'refuse'æ•°æ®äº†\n",
    "        # ä¸ºäº†ä¿è¯ Y è½´èŒƒå›´åˆç†ï¼Œæˆ‘ä»¬å°†ä¾èµ–äºåœ¨ layered_hist_with_labels å†…éƒ¨å¤„ç†è¿‡æ»¤åçš„ Y è½´èŒƒå›´\n",
    "        y_domain_dep = None\n",
    "\n",
    "        vals_ent = [df_feat[col].dropna().values for col in ent_cols if col in df_feat.columns]\n",
    "        valid_vals_ent = [v for v in vals_ent if v.size > 0]\n",
    "        x_extent_ent = _domain_x(np.concatenate(valid_vals_ent)) if valid_vals_ent and np.concatenate(valid_vals_ent).size > 0 else None\n",
    "        y_domain_ent = None\n",
    "\n",
    "        return x_extent_dep, y_domain_dep, x_extent_ent, y_domain_ent\n",
    "\n",
    "    x_extent_dep, y_domain_dep, x_extent_ent, y_domain_ent = get_global_extents(df_feat, dep_cols, ent_cols)\n",
    "\n",
    "    # Generate Dependency Depth Charts\n",
    "    charts_dep = []\n",
    "    # ğŸ”´ æ³¨æ„ï¼šè°ƒç”¨å‡½æ•°ä¿æŒä¸å˜ï¼Œä½†å…¶å†…éƒ¨é€»è¾‘å·²ç»ä¿®æ”¹ä¸ºåªå¤„ç† 'refuse'\n",
    "    if DEP_EN in df_feat.columns and LABEL_EN in df_feat.columns: charts_dep.append(layered_hist_with_labels(df_feat, LABEL_EN, DEP_EN, \"Avg Dependency Tree Depth (English)\", BINS, 320, 230, x_extent_dep, y_domain_dep))\n",
    "    if DEP_CN in df_feat.columns and LABEL_CN in df_feat.columns: charts_dep.append(layered_hist_with_labels(df_feat, LABEL_CN, DEP_CN, \"Avg Dependency Tree Depth (Chinese)\", BINS, 320, 230, x_extent_dep, y_domain_dep))\n",
    "    if DEP_MIX in df_feat.columns and LABEL_MIX in df_feat.columns: charts_dep.append(layered_hist_with_labels(df_feat, LABEL_MIX, DEP_MIX, \"Avg Dependency Tree Depth (Mixed)\", BINS, 320, 230, x_extent_dep, y_domain_dep))\n",
    "    row1 = alt.hconcat(*charts_dep).resolve_scale(color=\"independent\", y=\"independent\") if charts_dep else alt.Chart(pd.DataFrame()).mark_text(text=\"No Dependency Depth Charts\").properties(width=960, height=230)\n",
    "\n",
    "    # Generate Lexical Information Entropy Charts\n",
    "    charts_ent = []\n",
    "    if ENT_EN in df_feat.columns and LABEL_EN in df_feat.columns: charts_ent.append(layered_hist_with_labels(df_feat, LABEL_EN, ENT_EN, \"Lexical Information Entropy (English)\", BINS, 320, 230, x_extent_ent, y_domain_ent))\n",
    "    if ENT_CN in df_feat.columns and LABEL_CN in df_feat.columns: charts_ent.append(layered_hist_with_labels(df_feat, LABEL_CN, ENT_CN, \"Lexical Information Entropy (Chinese)\", BINS, 320, 230, x_extent_ent, y_domain_ent))\n",
    "    if ENT_MIX in df_feat.columns and LABEL_MIX in df_feat.columns: charts_ent.append(layered_hist_with_labels(df_feat, LABEL_MIX, ENT_MIX, \"Lexical Information Entropy (Mixed)\", BINS, 320, 230, x_extent_ent, y_domain_ent))\n",
    "    row2 = alt.hconcat(*charts_ent).resolve_scale(color=\"independent\", y=\"independent\") if charts_ent else alt.Chart(pd.DataFrame()).mark_text(text=\"No Lexical Entropy Charts\").properties(width=960, height=230)\n",
    "\n",
    "    # Combine charts and add the professional model name title\n",
    "    final_chart = (row1 & row2).properties(\n",
    "        title=alt.TitleParams(\n",
    "            text=f\"Prompt Structural Analysis of Refusal Behavior ({model_name})\",\n",
    "            anchor=\"middle\",\n",
    "            orient=\"bottom\",\n",
    "            dy=8,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"==================== Analysis Completed for Model: {model_name} ====================\\n\")\n",
    "    return final_chart\n",
    "\n",
    "# --- MAIN EXECUTION LOOP ---\n",
    "MODEL_SLUGS = list(MODEL_MAPPING.keys())\n",
    "print(f\"[INFO]: Total {len(MODEL_SLUGS)} models configured for analysis.\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1. å®šä¹‰å¹¶åˆ›å»ºä¿å­˜å›¾è¡¨çš„ç›®å½•\n",
    "SAVE_DIR = Path(\"images_2\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"[INFO]: Chart save directory created/verified: {SAVE_DIR.resolve()}\")\n",
    "\n",
    "all_charts = []\n",
    "\n",
    "# Iterate through the models\n",
    "for model_slug, model_name in MODEL_MAPPING.items():\n",
    "    chart = process_single_model(model_slug, model_name, BASE_DATA_DIR)\n",
    "\n",
    "    if chart:\n",
    "        all_charts.append(chart)\n",
    "        # å–æ¶ˆæ³¨é‡Šè¿™ä¸€è¡Œæ¥åœ¨ Notebook ä¸­ç«‹å³æ˜¾ç¤ºå›¾è¡¨\n",
    "        chart\n",
    "\n",
    "        # 2. ä¿å­˜å›¾è¡¨åˆ°æŒ‡å®šçš„ç›®å½•\n",
    "        file_name = f\"{model_slug}_analysis_chart_refusal_only.json\"\n",
    "        save_path = SAVE_DIR / file_name\n",
    "\n",
    "        try:\n",
    "            chart.save(save_path)\n",
    "            print(f\"[INFO]: Chart saved for {model_name} to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR]: Failed to save chart for {model_name} to {save_path}. Error: {e}\")\n",
    "\n",
    "print(f\"[INFO]: All {len(MODEL_SLUGS)} models processed.\")\n",
    "if not all_charts:\n",
    "    print(\"[ERROR]: No charts were generated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
